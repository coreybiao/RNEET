{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949c0347",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec3a2e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:48:49.343738Z",
     "start_time": "2024-07-22T07:48:47.124471Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as ln\n",
    "import math\n",
    "import argparse\n",
    "from scipy.sparse import diags\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "import time \n",
    "\n",
    "# torch packages \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55498674",
   "metadata": {},
   "source": [
    "# Graph Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c56f65e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:10:05.393662Z",
     "start_time": "2024-07-24T08:10:05.384685Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def graph_read(graph_name):\n",
    "    print(\"\\nGraph {} reading ... \".format(graph_name))\n",
    "    if graph_name == 'acm':\n",
    "        adj_mat = np.load(r'./dataset/acm/acm_adj.npy')\n",
    "        labels  = np.load(r'./dataset/acm/acm_label.npy')\n",
    "    if graph_name == 'cora':\n",
    "        adj_mat = np.load(r'./dataset/cora/cora_adj.npy')\n",
    "        labels  = np.load(r'./dataset/cora/cora_label.npy')\n",
    "    if graph_name == 'citeseer':\n",
    "        adj_mat = np.load(r'./dataset/citeseer/citeseer_adj.npy')\n",
    "        labels  = np.load(r'./dataset/citeseer/citeseer_label.npy')\n",
    "    if graph_name == 'wiki':\n",
    "        adj_mat = np.load(r'./dataset/wiki/wiki_adj.npy')\n",
    "        labels  = np.load(r'./dataset/wiki/wiki_label.npy')\n",
    "    if graph_name == 'amap':\n",
    "        adj_mat = np.load(r'./dataset/amap/amap_adj.npy')\n",
    "        labels  = np.load(r'./dataset/amap/amap_label.npy')\n",
    "        \n",
    "    print(\"Get information of graph {}.\\n\".format(graph_name))\n",
    "    \n",
    "    return nx.from_numpy_array(adj_mat), labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae1c21",
   "metadata": {},
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f8783b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:48:55.679962Z",
     "start_time": "2024-07-22T07:48:55.666997Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class MultiLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, dropout_rate=0.1):\n",
    "        super(MultiLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.relu(self.bn1(self.fc1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.bn2(self.fc2(out)))\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "        F.log_softmax(out, dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "def model_train(model, inputs, labels):\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "    \n",
    "    num_epochs = 200  \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "#         if (epoch+1) % 50 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}，Accuracy: {train_acc(outputs, labels):.2f}')\n",
    "            \n",
    "    return    \n",
    "\n",
    "\n",
    "def train_acc(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Compare the predicted labels with the ground truth (true labels)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate the accuracy by dividing the number of correct predictions by the total number of samples\n",
    "    total = labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def accuracy(model, inputs, labels):\n",
    "    outputs = model(inputs)\n",
    "    # Get the predicted labels (class indices) by taking the argmax along the output dimension\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Compare the predicted labels with the ground truth (true labels)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate the accuracy by dividing the number of correct predictions by the total number of samples\n",
    "    total = labels.size(0)\n",
    "    accuracy = correct / total\n",
    "#     print('Test accuracy:{:.2f}'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281f541",
   "metadata": {},
   "source": [
    "# Node Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005c964e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:49:05.929185Z",
     "start_time": "2024-07-22T07:49:05.905248Z"
    },
    "code_folding": [
     0,
     35
    ]
   },
   "outputs": [],
   "source": [
    "def ordered_list_centrality(graph, centrality_type):\n",
    "    \n",
    "    nodes = list(graph.nodes())\n",
    "    \n",
    "    if centrality_type == 'random':\n",
    "        print('\\nNodes sorted by Random:')\n",
    "        random.shuffle(nodes)\n",
    "        return nodes\n",
    "\n",
    "    if centrality_type == 'lc':\n",
    "        print('\\nNodes sorted by Local Neighbors:')\n",
    "        return  ordered_by_lc(graph, centrality_type = 'lc')\n",
    "    \n",
    "    # types of centrality measurement \n",
    "    centrality_measures = {\n",
    "        'pr': [nx.pagerank               , 'PageRank'],\n",
    "        'dg': [nx.degree_centrality      , 'Degree Centrality'],\n",
    "        'bt': [nx.betweenness_centrality , 'Betweenness Centrality'],\n",
    "        'cl': [nx.closeness_centrality   , 'Closeness Centrality'],\n",
    "        'eg': [nx.eigenvector_centrality , 'Eigenvector Centrality'] \n",
    "    }\n",
    "    # 字典 key-value : 节点-中心性\n",
    "    node_centrality = centrality_measures[centrality_type][0](graph)\n",
    "    \n",
    "    # 对 centrality 值进行排序\n",
    "    sorted_centrality = sorted(node_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "     \n",
    "    # 获取排序后的节点列表\n",
    "    sorted_nodes = [node for node, rank in sorted_centrality]\n",
    "\n",
    "    print('\\nNodes sorted by {}:'.format(centrality_measures[centrality_type][1]))\n",
    "\n",
    "    return sorted_nodes\n",
    "\n",
    "\n",
    "def ordered_list_centrality_probability(graph, centrality_type):\n",
    "    \n",
    "    nodes = list(graph.nodes())\n",
    "    \n",
    "    if centrality_type == 'random':\n",
    "        print('Nodes sorted by Random\\n')\n",
    "        random.shuffle(nodes)\n",
    "        return nodes\n",
    "\n",
    "    # types of centrality measurement \n",
    "    centrality_measures = {\n",
    "        'pr': [nx.pagerank               , 'PageRank'],\n",
    "        'dg': [nx.degree_centrality      , 'Degree Centrality'],\n",
    "        'bt': [nx.betweenness_centrality , 'Betweenness Centrality'],\n",
    "        'cl': [nx.closeness_centrality   , 'Closeness Centrality'],\n",
    "        'eg': [nx.eigenvector_centrality , 'Eigenvector Centrality'] \n",
    "    }\n",
    "    # 字典 key-value : 节点-中心性\n",
    "    node_centrality = centrality_measures[centrality_type][0](graph)\n",
    "    \n",
    "    ## 依概率排序\n",
    "    # 获取点的数量\n",
    "    num_points = len(nodes)\n",
    "    # 获取rank值总和\n",
    "    total_rank = sum(node_centrality.values())\n",
    "    # 计算每个点作为首位的概率\n",
    "    probabilities = {point: rank / total_rank for point, rank in node_centrality.items()}\n",
    "    # 生成排序后的点列表\n",
    "    sorted_nodes = sorted(probabilities, key=lambda x: random.random() ** (1/(probabilities[x] if probabilities[x] != 0 else 1)), reverse=True)\n",
    "\n",
    "    print('\\nNodes sorted by {}:'.format(centrality_measures[centrality_type][1]))\n",
    "\n",
    "    return sorted_nodes\n",
    "\n",
    "\n",
    "def ordered_by_lc(graph, centrality_type = 'lc'):\n",
    "    \n",
    "#     print('Select nodes by set cover on local clusters.')\n",
    "    nodes = list(graph.nodes())\n",
    "    epsilon = 10 ** - 2.5\n",
    "    node_local_cluster = defaultdict(list)\n",
    "    for node in nodes:\n",
    "        vec = forward_search(graph, node, epsilon)\n",
    "#         vec = backward_search(graph, node, epsilon)\n",
    "        non_zero_indices = [index for index, value in enumerate(vec) if value != 0]\n",
    "        node_local_cluster[node] = non_zero_indices\n",
    "        \n",
    "    selected_nodes = set_cover(node_local_cluster, 250)\n",
    "    \n",
    "    return list(selected_nodes)\n",
    "\n",
    "\n",
    "def set_cover(graph, K):\n",
    "    \n",
    "    universe = set.union(*map(set, graph.values()))\n",
    "\n",
    "    \n",
    "    selected_nodes = set()\n",
    "    iterations = 0\n",
    "\n",
    "    while universe and iterations < K:\n",
    "        \n",
    "        node_coverages = {node: len(set(universe).intersection(nodes)) for node, nodes in graph.items()}\n",
    "        selected_node = max(node_coverages, key=node_coverages.get)       \n",
    "        selected_nodes.add(selected_node)\n",
    "        universe -= set(graph[selected_node])\n",
    "      \n",
    "        del graph[selected_node]\n",
    "\n",
    "        iterations += 1\n",
    "        \n",
    "    print(len(universe))\n",
    "        \n",
    "    return selected_nodes\n",
    "\n",
    "\n",
    "def order_test():\n",
    "    \n",
    "    graph, _ = graph_read('cora')\n",
    "#     ordered_list = ordered_list_centrality(graph, 'eg')\n",
    "    \n",
    "    ordered_list = ordered_by_lc(graph, centrality_type = 'lc')\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# order_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e23147",
   "metadata": {},
   "source": [
    "## Anchor Nodes Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "764bdd97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T03:00:32.023770Z",
     "start_time": "2024-07-24T03:00:31.983876Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "def marginal_score(k, anchors: [int]) -> int:\n",
    "\n",
    "    print(k, anchors, sep = ': ')\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "marginal_score(3, [1, 2])\n",
    "\n",
    "\n",
    "def anchor_nodes_v1(graph, topk, k) -> []:\n",
    "   \n",
    "    nodes = list(graph.nodes())\n",
    "    epsilon = 10 ** -2.5\n",
    "    node_local_cluster = defaultdict(list)\n",
    "    for node in nodes:\n",
    "        vec = forward_search(graph, node, epsilon)\n",
    "        non_zero_indices = [index for index, value in enumerate(vec) if value != 0]\n",
    "        node_local_cluster[node] = non_zero_indices            # indices of radiated neighbors\n",
    "    \n",
    "    start_time = time.time()                     \n",
    "    node_actnum = {}\n",
    "    node_mscore:[(int, int)] =  []               # [(node, mscore)]\n",
    "    for node in nodes: \n",
    "#         node_actnum[node] = 0\n",
    "        node_mscore.append((node, k*len(node_local_cluster[node]))) # initial marginal score for each node\n",
    "    \n",
    "    anchors = []\n",
    "    node_mscore.sort(key = lambda x: x[1], reverse = True)\n",
    "    anchors.append(node_mscore[0][0])\n",
    "\n",
    "#     radiated_nodes = []   \n",
    "    \n",
    "    for i in range(1, topk):\n",
    "        \n",
    "        for rad_node in node_local_cluster[anchors[-1]]:              \n",
    "            actnum = node_actnum.get(rad_node, 0)+1          \n",
    "            node_actnum[rad_node] = min(k, actnum)           \n",
    "#             if node_actnum[node] != 1:\n",
    "#                 radiated_nodes.append(nodes)                      \n",
    "        \n",
    "        anchor_num  = len(anchors)\n",
    "        # marginal score update\n",
    "        for j in range(anchor_num, len(node_mscore)):\n",
    "            nodej_mscore = 0                                           \n",
    "            for rad_node in node_local_cluster[node_mscore[j][0]]:     \n",
    "                actnum = node_actnum.get(rad_node, 0)                   \n",
    "                nodej_mscore += (k - actnum)                            \n",
    "            node_mscore[j] = (node_mscore[j][0], nodej_mscore)          \n",
    "            \n",
    "        node_mscore.sort(key = lambda x: x[1], reverse = True)\n",
    "        anchors.append(node_mscore[i][0])\n",
    "        \n",
    "    end_time = time.time()                 \n",
    "     \n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execuation time：\", execution_time, \"seconds\")\n",
    "    \n",
    "#     print('anchor set:\\n',anchors, len(anchors))\n",
    "    return execution_time\n",
    "\n",
    "\n",
    "def anchor_nodes_optmz_v1(graph, topk, k) -> []:\n",
    "   \n",
    "    nodes = list(graph.nodes())\n",
    "    epsilon = 10 ** -2.5\n",
    "    node_local_cluster = defaultdict(list)\n",
    "    node_mscore:[(int, int)] =  []               # [(node, mscore)]\n",
    "\n",
    "    for node in nodes:\n",
    "        vec = forward_search(graph, node, epsilon)\n",
    "        non_zero_indices = [index for index, value in enumerate(vec) if value != 0]\n",
    "        node_local_cluster[node] = non_zero_indices            # indices of radiated neighbors\n",
    "        node_mscore.append((node, k*len(non_zero_indices)))    # initial marginal score for each node\n",
    "    \n",
    "    start_time = time.time()                      \n",
    "    node_actnum = {}\n",
    "    anchors = []\n",
    "    node_mscore.sort(key = lambda x: x[1], reverse = True)\n",
    "    anchors.append(node_mscore[0][0])\n",
    "    \n",
    "    for i in range(1, topk):\n",
    "         \n",
    "        for rad_node in node_local_cluster[anchors[-1]]:               \n",
    "            actnum = node_actnum.get(rad_node, 0)+1                    \n",
    "            node_actnum[rad_node] = min(k, actnum)                     \n",
    "             \n",
    "        ## Step 1 : 找到第m个小于当前第二个的\n",
    "          # 先更新marginal score对第i个点在node_mscore中\n",
    "        nodei_mscore = 0       # 对其marginal score进行重新计算\n",
    "        for rad_node in node_local_cluster[node_mscore[i][0]]:      \n",
    "            actnum = node_actnum.get(rad_node, 0)\n",
    "            nodei_mscore += (k - actnum)                            \n",
    "        node_mscore[i] = (node_mscore[i][0], nodei_mscore)          \n",
    "           \n",
    "        m = i+1\n",
    "        for j in range(i+1, len(node_mscore)):\n",
    "            if node_mscore[j][1] < nodei_mscore:\n",
    "                m = j\n",
    "                break\n",
    "        \n",
    "        ## Step 2 : 重新更新排序\n",
    "        for j in range(i+1, m+1):\n",
    "            nodej_mscore = 0        \n",
    "            for rad_node in node_local_cluster[node_mscore[j][0]]:          \n",
    "                actnum = node_actnum.get(rad_node, 0)\n",
    "                nodej_mscore += (k - actnum)                                \n",
    "            node_mscore[j] = (node_mscore[j][0], nodej_mscore)              \n",
    "\n",
    "        node_mscore[i:m] = sorted(node_mscore[i:m], key = lambda x: x[1], reverse = True)\n",
    "        anchors.append(node_mscore[i][0])  \n",
    "        \n",
    "    \n",
    "    end_time = time.time()\n",
    "   \n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execuation time：\", execution_time, \"secondes\")\n",
    "    \n",
    "#     print('Anchor Set:\\n',anchors, len(anchors))\n",
    "#     print([node[0] for node in node_mscore[0:len(anchors)]])\n",
    "#     print([node[1] for node in node_mscore[0:len(anchors)]])\n",
    "   \n",
    "    return execution_time\n",
    "\n",
    "\n",
    "def anchor_nodes(graph, topk, k) -> []:\n",
    "    \n",
    "    nodes = list(graph.nodes())\n",
    "    epsilon = 10 ** -2\n",
    "    node_local_cluster = defaultdict(list)\n",
    "    for node in nodes:\n",
    "        vec = forward_search(graph, node, epsilon)\n",
    "        non_zero_indices = [index for index, value in enumerate(vec) if value != 0]\n",
    "        node_local_cluster[node] = non_zero_indices            # indices of radiated neighbors\n",
    "    \n",
    "    start_time = time.time()                      \n",
    "    node_actnum = {}\n",
    "    node_mscore:[(int, int)] =  []                \n",
    "    for node in nodes: \n",
    "        node_actnum[node] = 0\n",
    "        node_mscore.append((node, k*len(node_local_cluster[node]))) # initial marginal score for each node\n",
    "    \n",
    "    anchors = []\n",
    "    node_mscore.sort(key = lambda x: x[1], reverse = True)\n",
    "    anchors.append(node_mscore[0][0])\n",
    "\n",
    "    radiated_nodes = []    \n",
    "    \n",
    "    for i in range(1, topk):\n",
    "         \n",
    "        for node in node_local_cluster[anchors[-1]]:               \n",
    "            node_actnum[node] = min(k, node_actnum[node]+1)       \n",
    "            if node_actnum[node] != 1:\n",
    "                radiated_nodes.append(nodes)                       \n",
    "        \n",
    "        anchor_num  = len(anchors)\n",
    "         \n",
    "        for j in range(anchor_num, len(node_mscore)):\n",
    "            nodej_mscore = 0                                             \n",
    "            for node in node_local_cluster[node_mscore[j][0]]:          \n",
    "                nodej_mscore += (k - node_actnum[node])                 \n",
    "            node_mscore[j] = (node_mscore[j][0], nodej_mscore)         \n",
    "            \n",
    "        node_mscore.sort(key = lambda x: x[1], reverse = True)\n",
    "        anchors.append(node_mscore[i][0])\n",
    "        \n",
    "    end_time = time.time()                  \n",
    "    \n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Executation time：\", execution_time, \"secondes\")\n",
    "    \n",
    "#     print('anchor set: ',anchors, len(anchors))\n",
    "    return anchors\n",
    "\n",
    "\n",
    "def anchor_nodes_optmz(graph, topk, k) -> []:\n",
    "   \n",
    "    nodes = list(graph.nodes())\n",
    "    epsilon = 10 ** -2\n",
    "    node_local_cluster = defaultdict(list)\n",
    "    for node in nodes:\n",
    "        vec = forward_search(graph, node, epsilon)\n",
    "        non_zero_indices = [index for index, value in enumerate(vec) if value != 0]\n",
    "        node_local_cluster[node] = non_zero_indices            # indices of radiated neighbors\n",
    "    \n",
    "    node_actnum = {}\n",
    "    node_mscore:[(int, int)] =  []            # [(node, mscore)]\n",
    "    for node in nodes: \n",
    "        node_actnum[node] = 0\n",
    "        node_mscore.append((node, k*len(node_local_cluster[node]))) # initial marginal score for each node\n",
    "    \n",
    "    \n",
    "    start_time = time.time()                  \n",
    "    anchors = []\n",
    "    node_mscore.sort(key = lambda x: x[1], reverse = True)\n",
    "    anchors.append(node_mscore[0][0])\n",
    "\n",
    "    radiated_nodes = []    \n",
    "    \n",
    "    for i in range(1, topk):\n",
    "         \n",
    "        for node in node_local_cluster[anchors[-1]]:               \n",
    "            node_actnum[node] = min(k, node_actnum[node]+1)        \n",
    "            if node_actnum[node] != 1:\n",
    "                radiated_nodes.append(nodes)                       \n",
    "        \n",
    "        ## Step 1 :\n",
    "         \n",
    "        nodei_mscore = 0       #  marginal score recompute\n",
    "        for node in node_local_cluster[node_mscore[i][0]]:          \n",
    "            nodei_mscore += (k - node_actnum[node])                 \n",
    "        node_mscore[i] = (node_mscore[i][0], nodei_mscore)          \n",
    "           \n",
    "        m = i+1\n",
    "        for j in range(i+1, len(node_mscore)):\n",
    "            if node_mscore[j][1] < nodei_mscore:\n",
    "                m = j\n",
    "                break\n",
    "        \n",
    "        ## Step 2 :\n",
    "        if m == i+1:                                                \n",
    "            anchors.append(node_mscore[i][0])\n",
    "        else:        \n",
    "            for j in range(i+1, m+1):\n",
    "                nodej_mscore = 0        \n",
    "                for node in node_local_cluster[node_mscore[j][0]]:          \n",
    "                    nodej_mscore += (k - node_actnum[node])                 \n",
    "                node_mscore[j] = (node_mscore[j][0], nodej_mscore)          \n",
    "            \n",
    "            node_mscore[i:m] = sorted(node_mscore[i:m], key = lambda x: x[1], reverse = True)\n",
    "            anchors.append(node_mscore[i][0])\n",
    "    \n",
    "    end_time = time.time()\n",
    "     \n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execuation time：\", execution_time, \"secondes\")\n",
    "    \n",
    "#     print('anchor set:\\n',anchors, len(anchors))\n",
    "    return execution_time\n",
    "\n",
    "  \n",
    "def anchor_select_test()->None: \n",
    "    graph, labels = graph_read('citeseer')\n",
    "    nodes = list(graph.nodes())\n",
    "    print(type(nodes[0]))\n",
    "    \n",
    "    for k in [1,2,3,4,5]:\n",
    "        print('K is:',k)\n",
    "        for topk in [50, 100, 150, 200, 250]:\n",
    "            print('Anchor scale is:', topk)\n",
    "            runtime = anchor_nodes(graph, topk, k)\n",
    "            opt_runtime = anchor_nodes_optmz(graph, topk, k)\n",
    "            print('Optimize：', round(runtime/opt_runtime,2),'times')\n",
    "            print()\n",
    "#             runtime = anchor_nodes_v1(graph, topk, k)\n",
    "#             opt_runtime = anchor_nodes_optmz_v1(graph, topk, k)\n",
    "#             print('Optimize：', round(runtime/opt_runtime,2),'times')\n",
    "    return\n",
    "\n",
    "\n",
    "# anchor_select_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ccae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcbc91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e742c247",
   "metadata": {},
   "source": [
    "# PPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4109f",
   "metadata": {},
   "source": [
    "## Forward & Backward Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8cf0618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T03:00:36.608508Z",
     "start_time": "2024-07-24T03:00:36.589558Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def forward_search(graph, source_node, epsilon):\n",
    "    alpha = 0.15\n",
    "    nodes = list(graph.nodes)\n",
    "    \n",
    "    rf = {node: 0.0 for node in nodes}\n",
    "    rf[source_node] = 1.0\n",
    "\n",
    "    ppr_vector = {node: 0.0 for node in nodes}\n",
    "    \n",
    "    if graph.degree(source_node) == 0:\n",
    "        ppr_vector[source_node] = 1.0\n",
    "        return list(ppr_vector.values())\n",
    "    \n",
    "    queue = [source_node]\n",
    "    \n",
    "    iterate_num = 0\n",
    "    while queue:\n",
    "        iterate_num += 1\n",
    "        for u in queue:\n",
    "            if rf[u]/graph.degree(u) >= epsilon:\n",
    "                for v in list(graph.neighbors(u)):\n",
    "                    rf[v] += (1-alpha) * (rf[u]/graph.degree(u))\n",
    "                    if rf[v]/graph.degree(v) >= epsilon and v not in queue:\n",
    "                        queue.append(v)\n",
    "                ppr_vector[u] += alpha * rf[u]\n",
    "                rf[u] = 0\n",
    "            else: \n",
    "                queue.remove(u)\n",
    "    return list(ppr_vector.values())\n",
    "\n",
    "\n",
    "def backward_search(graph, target_node, epsilon):\n",
    "    alpha = 0.15\n",
    "    nodes = list(graph.nodes)    \n",
    "\n",
    "    rb = {node:0.0 for node in nodes}\n",
    "    rb[target_node] = 1\n",
    "    \n",
    "    t_ppr_vec = {node :0.0 for node in nodes}\n",
    "    \n",
    "    if graph.degree(target_node) == 0:\n",
    "        t_ppr_vec[target_node] = 1.0\n",
    "        return list(t_ppr_vec.values())\n",
    "    \n",
    "    queue = [target_node]\n",
    "    \n",
    "    while queue:\n",
    "        for v in queue:\n",
    "            if rb[v] > epsilon: \n",
    "                for u in list(graph.neighbors(v)):\n",
    "                    rb[u] += (1-alpha) * (rb[v]/graph.degree(u))\n",
    "                    if rb[u] > epsilon and u not in queue:\n",
    "                        queue.append(u)\n",
    "                t_ppr_vec[v] += alpha * rb[v]\n",
    "                rb[v] = 0\n",
    "            else:\n",
    "                queue.remove(v)\n",
    "    \n",
    "    return list(t_ppr_vec.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20333a81",
   "metadata": {},
   "source": [
    "## PPR Vector Support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "611b44f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T03:00:56.655755Z",
     "start_time": "2024-07-24T03:00:56.644784Z"
    },
    "code_folding": [
     1
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def ppr_support(graph_name, epsilon = 1e-3):\n",
    "    \n",
    "    graph, _ = graph_read(graph_name)\n",
    "    nodes = list(graph.nodes)\n",
    "    n = len(nodes)\n",
    "    epsilon = 1/n\n",
    "    \n",
    "    \n",
    "     \n",
    "    isolated_nodes = list(nx.isolates(graph))\n",
    "    print('NO. of nodes is {}, NO. of isolated nodes is : {}'.format(len(nodes), len(isolated_nodes)))\n",
    "    \n",
    "#     random.shuffle(nodes)\n",
    "    node_supports = []\n",
    "    for node in nodes:\n",
    "        vec = forward_search(graph, node, epsilon)\n",
    "#         vec = backward_search(graph, node, epsilon)\n",
    "        non_zero_indices = [index for index, value in enumerate(vec) if value != 0.0]\n",
    "        if len(non_zero_indices) > 10:\n",
    "            print(sum(vec))\n",
    "#         print(len(non_zero_indices))\n",
    "        node_supports.append(len(non_zero_indices))\n",
    "    \n",
    "     \n",
    "    support_ave = sum(node_supports) / len(node_supports)\n",
    "     \n",
    "    median = statistics.median(node_supports)\n",
    "     \n",
    "    max_supprot, min_support = max(node_supports), min(node_supports)\n",
    "\n",
    "    print('Support average:{}, median:{}, max:{}, min:{}'.format(support_ave,median,max_supprot,min_support))\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for num in node_supports:\n",
    "        if num > 100:\n",
    "            count += 1\n",
    "\n",
    "    print(count)\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# ppr_support('wiki', epsilon = 1e-5)\n",
    "# ppr_support('cora', epsilon = 1e-3)\n",
    "# ppr_support('amac', epsilon = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e65311",
   "metadata": {},
   "source": [
    "# Node Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b2795",
   "metadata": {},
   "source": [
    "## RNEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efd1ae51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:44:05.225577Z",
     "start_time": "2024-07-24T07:44:05.207625Z"
    }
   },
   "outputs": [],
   "source": [
    "def rneet(graph, emb_dim = 50, K = 1, Theta = 2, Epsilon = 1e-2):\n",
    "    \n",
    "    n = graph.number_of_nodes()\n",
    "    \n",
    "    anchor_scale = 500\n",
    "    repnodes, anchor_mscore = anchor_nodes_marginal(graph, topk = anchor_scale, k = K, epsilon = Epsilon)\n",
    "    mscore_ave = [sum(anchor_mscore[i*20:i*20+20])/20 for i in range(int(anchor_scale/20))]\n",
    "    node_num = first_less_theta_index(mscore_ave, Theta*K)*20+20     # 锚节点规模\n",
    "    print('\\tMarginal Score Average: {}'.format(mscore_ave))\n",
    "    print('\\tAnchor node scale: {}({:.2%}) with damping factor K: {}'.format(node_num, node_num/n, K))\n",
    "    \n",
    "    similarity_matrix = np.zeros((n, emb_dim+20))\n",
    "    \n",
    "    pagerank = np.array(list(nx.pagerank(graph).values()))\n",
    "    for node in repnodes[0:emb_dim+20]:\n",
    "        ##  Negative sampling(PR-based): x = log(x) / PR(x)\n",
    "        pr_vec = np.array(list(nx.pagerank(graph, personalization={node: 1}).values()))\n",
    "        pr_vec = pr_vec / pagerank                              \n",
    "        pr_vec = np.nan_to_num(pr_vec, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        vec_nce = np.where(pr_vec < 1, 0,                        # 如果 x 小于1，将log(x)设置为0\n",
    "                      np.log(pr_vec ))                           # 否则设为x = log(x)\n",
    "        similarity_matrix[:,repnodes.index(node)] = vec_nce\n",
    "    \n",
    "    iterations = int((node_num-(emb_dim+20))/20)\n",
    "    print('Number of iterations:{}'.format(iterations))\n",
    "    \n",
    "    \n",
    "    for i in range(0, iterations):\n",
    "        print('Iteration {}'.format(i))\n",
    "               \n",
    "        mat_u, vec_sigma, mat_v = ln.svd(similarity_matrix, full_matrices=False)\n",
    "        \n",
    "        squared_sv_center = vec_sigma[math.floor(emb_dim)] ** 2\n",
    "        # update sigma to shrink the row norms\n",
    "        sigma_tilda = [(0.0 if d < 0.0 else math.sqrt(d)) for d in (vec_sigma ** 2 - squared_sv_center)]\n",
    "#         print(sigma_tilda)\n",
    "        # update matrix B where at 20 rows are all zero\n",
    "        similarity_matrix = np.dot(mat_u, np.diagflat(sigma_tilda))\n",
    "        \n",
    "        for j in range(20):\n",
    "#             print('Node index : {}'.format(i*emb_dim+j))\n",
    "            node = repnodes[emb_dim + 20 + 20*i+j]\n",
    "            ppr_vec = np.array(list(nx.pagerank(graph, personalization={node: 1}).values()))\n",
    "            pr_vec = pr_vec / pagerank                                \n",
    "            pr_vec = np.nan_to_num(pr_vec, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            vec_nce = np.where(pr_vec < 1, 0,                         \n",
    "                          np.log(pr_vec ))                           \n",
    "            similarity_matrix[:,emb_dim+j] = vec_nce\n",
    "    \n",
    "    mat_u, vec_sigma, mat_v = ln.svd(similarity_matrix, full_matrices=False)\n",
    "    diag = np.power(vec_sigma[0 : emb_dim], 0.5)\n",
    "    diag = diags(diag, 0)\n",
    "    emb_mat = mat_u[:, 0 : emb_dim] @ diag\n",
    "    \n",
    "    return emb_mat\n",
    "\n",
    "\n",
    "def rneet_test(graph_name = 'cora'):\n",
    "    \n",
    "    graph,_ = graph_read(graph_name)\n",
    "    \n",
    "    embeddings = rneet(graph, emb_dim = 50, K = 1, Theta = 6, Epsilon = 1e-2)\n",
    "    print('Dimension is : {}'.format(embeddings.shape[1]))\n",
    "#     print(embeddings[0])\n",
    "    \n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "# rneet_test(graph_name = 'cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab00f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec08f44d",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ec869",
   "metadata": {},
   "source": [
    "## Classification V.S. Negative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7e22fe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T02:57:18.086163Z",
     "start_time": "2024-07-24T02:57:18.060232Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cls_nd(graph_name: str, neg_dis = 'uniform', node_num = 50):\n",
    "\n",
    "     \n",
    "    graph, labels = graph_read(graph_name)\n",
    "    nodes = list(graph.nodes())\n",
    "    cls_num = len(np.unique(labels))\n",
    "    labels_torch = torch.tensor(labels).long()\n",
    "    n = graph.number_of_nodes()\n",
    "    \n",
    "     \n",
    "    train_percent = 0.1\n",
    "    test_percent = 1 - train_percent * 2\n",
    "    train_num = int(n * train_percent)\n",
    "    test_num  = int(n * test_percent )\n",
    "        \n",
    "    repnodes = nodes[:node_num]\n",
    "    \n",
    "    n, m = graph.number_of_nodes(), graph.number_of_edges()\n",
    "    \n",
    "     \n",
    "    similarity_matrix = np.zeros((n, len(repnodes)))\n",
    "    \n",
    "    if neg_dis == 'no':\n",
    "        print('Without Negative Sampling. ')\n",
    "        for node in repnodes:\n",
    "            pr_vec = np.array(list(nx.pagerank(graph, personalization={node: 1}).values()))\n",
    "            similarity_matrix[:,repnodes.index(node)] = pr_vec\n",
    "    \n",
    "    elif neg_dis == 'uniform':\n",
    "        print('Negative Samples Are Uniformly Distrubuted. ')\n",
    "        for node in repnodes:\n",
    "            pr_vec = np.array(list(nx.pagerank(graph, personalization={node: 1}).values()))\n",
    "            ##  Negative sampling: x = log(x) + log(n) when x*n > 0\n",
    "            vec_nce = np.where(pr_vec * n < 1, 0,          \n",
    "                          np.log(pr_vec * n + 1e-20))      \n",
    "            similarity_matrix[:,repnodes.index(node)] = vec_nce\n",
    "    else:\n",
    "        print('Negative Samples Are Biased Distrubuted. ')\n",
    "         \n",
    "        pagerank = np.array(list(nx.pagerank(graph).values()))\n",
    "        for node in repnodes:\n",
    "            ##  Negative sampling(PR-based): x = log(x) / PR(x)\n",
    "            pr_vec = np.array(list(nx.pagerank(graph, personalization={node: 1}).values()))\n",
    "            pr_vec = pr_vec / pagerank                    \n",
    "            vec_nce = np.where(pr_vec < 1, 0,             \n",
    "                          np.log(pr_vec ))                \n",
    "            similarity_matrix[:,repnodes.index(node)] = vec_nce\n",
    "        \n",
    "    # SVD decomposition\n",
    "    mat_u, vec_sigma, mat_v = ln.svd(similarity_matrix, full_matrices=False)\n",
    "    d = 50\n",
    "    diag = np.power(vec_sigma[0 : d], 0.5)\n",
    "    diag = diags(diag, 0)\n",
    "    embeddings = mat_u[:, 0 : d] @ diag\n",
    "    dim = max(embeddings.shape[1], 50)\n",
    "    node_embeddings = torch.tensor(embeddings).to(torch.float32)\n",
    "\n",
    "    print(similarity_matrix.shape)\n",
    "    \n",
    "    similarity_matrix = torch.tensor(similarity_matrix).float()   \n",
    "    \n",
    "    labels_torch = torch.tensor(labels).long()\n",
    "    cls_num = len(np.unique(labels))\n",
    "    \n",
    "    run_num = 10\n",
    "    acc = 0\n",
    "    for _ in tqdm(range(run_num), desc='Training Iterations',ncols=50):\n",
    "        # Samples selection\n",
    "        node_random_sort = [i for i in range(n)]   \n",
    "        random.shuffle(node_random_sort)                                        \n",
    "        ## train and test samples\n",
    "        train_node = node_random_sort[0 : train_num]                            \n",
    "        test_node = node_random_sort[train_num :train_num + test_num]            \n",
    "\n",
    "        model = MultiLayerNet(input_size = dim, num_classes = cls_num, dropout_rate = 0.3)\n",
    "        model.train()\n",
    "        model_train(model, node_embeddings[train_node], labels_torch[train_node])\n",
    "        model.eval()\n",
    "        acc += accuracy(model, node_embeddings[test_node], labels_torch[test_node])\n",
    "        \n",
    "    print('Node classification accuracy: {:.2f}% with {} selected nodes'.\n",
    "          format((acc/run_num)*100, len(repnodes)))\n",
    "\n",
    "    return \n",
    "\n",
    "for num in [500, 1000, 1500, 2000, 2500, 7000]:\n",
    "# for num in [2900]:\n",
    "    print(\"*** \"+\"Node number is: \"+num+\" ***\")\n",
    "    graph_name = 'amap'\n",
    "    cls_nd(graph_name, neg_dis = 'no', node_num = num)\n",
    "    cls_nd(graph_name, neg_dis = 'uniform', node_num = num)\n",
    "    cls_nd(graph_name, neg_dis = 'pr-based', node_num = num)\n",
    "    print(\"*** \"+ \"Finished!\"+ \" ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e5b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T14:50:06.158712Z",
     "start_time": "2024-05-06T14:25:06.747270Z"
    },
    "code_folding": [
     13,
     69
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45a151ec",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b8f7428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:44:49.965859Z",
     "start_time": "2024-07-24T07:44:49.949901Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def visualization(graph_name = 'cora', node_num = 10, neg_dis = 'no', rdm = 'True'):\n",
    "\n",
    "   \n",
    "    graph, labels = graph_read(graph_name)\n",
    "    nodes = list(graph.nodes())\n",
    "    cls_num = len(np.unique(labels))\n",
    "    labels_torch = torch.tensor(labels).long()\n",
    "    n = graph.number_of_nodes()\n",
    "    \n",
    "    if rdm == 'True':\n",
    "#         repnodes = ordered_list_centrality_probability(graph, \"pr\")[-50:]\n",
    "        repnodes = random.sample(nodes, 50)\n",
    "        \n",
    "    else:\n",
    "        repnodes = anchor_nodes(graph, 50, 1)\n",
    "#         repnodes = ordered_list_centrality_probability(graph, \"pr\")[0:50]\n",
    "#         repnodes = ordered_list_centrality_probability(graph, \"pr\")[0:node_num]\n",
    "    \n",
    "    \n",
    "    n, m = graph.number_of_nodes(), graph.number_of_edges()\n",
    "    \n",
    "    \n",
    "    similarity_matrix = np.zeros((n, len(repnodes)))\n",
    "    \n",
    "    \n",
    "    if neg_dis == 'no':\n",
    "        print('Without Negative Sampling. ')\n",
    "        for node in repnodes:\n",
    "            pr_vec = np.array(list(nx.pagerank(graph, personalization={node: 1}).values()))\n",
    "            similarity_matrix[:,repnodes.index(node)] = pr_vec\n",
    "    \n",
    "    elif neg_dis == 'uniform':\n",
    "        print('Negative Samples Are Uniformly Distrubuted. ')\n",
    "        for node in repnodes:\n",
    "            pr_vec = np.array(list(nx.pagerank(graph, personalization={node: 1}).values()))\n",
    "            ##  Negative sampling: x = log(x) + log(n) when x*n > 0\n",
    "            vec_nce = np.where(pr_vec * n < 1, 0,         \n",
    "                          np.log(pr_vec * n + 1e-20))     \n",
    "            similarity_matrix[:,repnodes.index(node)] = vec_nce\n",
    "    else:\n",
    "        print('Negative Samples Are Biased Distrubuted. ')\n",
    "         \n",
    "        pagerank = np.array(list(nx.pagerank(graph).values()))\n",
    "        for node in repnodes:\n",
    "            ##  Negative sampling(PR-based): x = log(x) / PR(x)\n",
    "            pr_vec = np.array(list(nx.pagerank(graph, personalization={node: 1}).values()))\n",
    "            pr_vec = pr_vec / pagerank                   \n",
    "            vec_nce = np.where(pr_vec < 1, 0,            \n",
    "                          np.log(pr_vec ))               \n",
    "            similarity_matrix[:,repnodes.index(node)] = vec_nce\n",
    "        \n",
    "    mat_u, vec_sigma, mat_v = ln.svd(similarity_matrix, full_matrices=False)\n",
    "    \n",
    "    dimension = 50\n",
    "    diag = np.power(vec_sigma[0 : dimension], 0.5)\n",
    "    diag = diags(diag, 0)\n",
    "    embeddings_matrix = mat_u[:, 0 : dimension] @ diag\n",
    "    tsne = TSNE(n_components=2, learning_rate='auto', random_state=42)\n",
    "    X_2d = tsne.fit_transform(embeddings_matrix)\n",
    "    \n",
    "#     pca = PCA(n_components=2)\n",
    "#     X_2d = pca.fit_transform(embeddings_matrix)\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "#     plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='jet', s=0.5)\n",
    "    \n",
    "   \n",
    "    random_indices = random.sample(range(len(X_2d)), 1000)\n",
    "    random_points = X_2d[random_indices]\n",
    "    random_labels = labels[random_indices]\n",
    "    plt.scatter(random_points[:, 0], random_points[:, 1], c=random_labels, cmap='jet', s=2)  # cmap为色图选择Jet\n",
    "\n",
    "    \n",
    "    # 隐藏横纵坐标\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "#     plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc185d6d",
   "metadata": {},
   "source": [
    "## Radiation marginal and performance marginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5dade84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T08:21:13.136101Z",
     "start_time": "2024-07-23T08:21:13.111167Z"
    },
    "code_folding": [
     54
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def anchor_nodes_marginal(graph, topk, k, epsilon = 10 ** -2) -> []:\n",
    "    \n",
    "    nodes = list(graph.nodes())\n",
    "#     epsilon = 10 ** -2\n",
    "    node_local_cluster = defaultdict(list)\n",
    "    for node in nodes:\n",
    "        vec = forward_search(graph, node, epsilon)\n",
    "        non_zero_indices = [index for index, value in enumerate(vec) if value != 0]\n",
    "        node_local_cluster[node] = non_zero_indices            # indices of radiated neighbors\n",
    "    \n",
    "    start_time = time.time()                      \n",
    "    node_actnum = {}\n",
    "    node_mscore:[(int, int)] =  []               # [(node, mscore)]\n",
    "    for node in nodes: \n",
    "        node_actnum[node] = 0\n",
    "        node_mscore.append((node, k*len(node_local_cluster[node]))) # initial marginal score for each node\n",
    "    \n",
    "    anchors = []\n",
    "    node_mscore.sort(key = lambda x: x[1], reverse = True)\n",
    "    anchors.append(node_mscore[0][0])\n",
    "\n",
    "    anchor_mscore = []\n",
    "    anchor_mscore.append(node_mscore[0][1])\n",
    "    \n",
    "    radiated_nodes = []   \n",
    "    \n",
    "    for i in range(1, topk):\n",
    "         \n",
    "        for node in node_local_cluster[anchors[-1]]:              \n",
    "            node_actnum[node] = min(k, node_actnum[node]+1)        \n",
    "            if node_actnum[node] != 1:\n",
    "                radiated_nodes.append(nodes)                       \n",
    "        \n",
    "        anchor_num  = len(anchors)\n",
    "         \n",
    "        for j in range(anchor_num, len(node_mscore)):\n",
    "            nodej_mscore = 0                                             \n",
    "            for node in node_local_cluster[node_mscore[j][0]]:          \n",
    "                nodej_mscore += (k - node_actnum[node])                 \n",
    "            node_mscore[j] = (node_mscore[j][0], nodej_mscore)          \n",
    "            \n",
    "        node_mscore.sort(key = lambda x: x[1], reverse = True)\n",
    "        anchors.append(node_mscore[i][0])\n",
    "        anchor_mscore.append(node_mscore[i][1])\n",
    "        \n",
    "    end_time = time.time()                  \n",
    "     \n",
    "    execution_time = end_time - start_time\n",
    "    print(\"\\tExecuation：\", execution_time, \"secondes\")\n",
    "    \n",
    "#     print('anchor set: ',anchors, len(anchors))\n",
    "    return anchors, anchor_mscore\n",
    "\n",
    "\n",
    "def first_zero_index(lst):\n",
    "    for i, item in enumerate(lst):\n",
    "        if item == 0:\n",
    "            return i\n",
    "    return None  # 如果列表中没有非零项，则返回 None\n",
    "\n",
    "\n",
    "def first_less_theta_index(lst, theta):\n",
    "    for i, item in enumerate(lst):\n",
    "        if item <= theta:\n",
    "            return i\n",
    "    return None   \n",
    "\n",
    "\n",
    "def marginal_effect_performance(graph, labels, K = 1, Theta = 15, anchor_scale = 500, Epsilon = 10**-2):\n",
    "     \n",
    "    nodes = list(graph.nodes())\n",
    "    cls_num = len(np.unique(labels))\n",
    "    labels_torch = torch.tensor(labels).long()\n",
    "    n = graph.number_of_nodes()\n",
    "    \n",
    "     \n",
    "    train_percent = 0.1\n",
    "    test_percent = 1 - train_percent * 2\n",
    "    train_num = int(n * train_percent)\n",
    "    test_num  = int(n * test_percent )\n",
    "    \n",
    "    repnodes, anchor_mscore = anchor_nodes_marginal(graph, topk = anchor_scale, k = K, epsilon = Epsilon)\n",
    "    mscore_ave = [sum(anchor_mscore[i*20:i*20+20])/20 for i in range(int(anchor_scale/20))]\n",
    "    node_num = first_less_theta_index(mscore_ave, Theta*K)*20+20      \n",
    "    print('\\tMarginal Score Average: {}'.format(mscore_ave))\n",
    "    print('\\tAnchor node scale: {}({:.2%}) with damping factor K: {}'.format(node_num, node_num/n, K))\n",
    "    \n",
    "    node_local_cluster = defaultdict(list)\n",
    "    for node in nodes:\n",
    "        vec = forward_search(graph, node, epsilon = Epsilon)\n",
    "        non_zero_indices = [index for index, value in enumerate(vec) if value != 0]\n",
    "        node_local_cluster[node] = non_zero_indices               # indices of radiated neighbors\n",
    "    \n",
    "    coverage = []\n",
    "    cover_margin = []\n",
    "    performance = [] \n",
    "    performance_margin = []\n",
    "    for node in repnodes[0: node_num]:\n",
    "        coverage = list(set(coverage) | set(node_local_cluster[node]))\n",
    "    print(\"\\tTotal {}({:.2%}) nodes are covered.\".format(len(coverage), len(coverage)/n))\n",
    "    \n",
    "     \n",
    "    similarity_matrix = np.zeros((n, node_num))\n",
    "\n",
    "    \n",
    "    pagerank = np.array(list(nx.pagerank(graph).values()))\n",
    "    for node in repnodes[0:node_num]:\n",
    "        ##  Negative sampling(PR-based): x = log(x) / PR(x)\n",
    "        pr_vec = np.array(list(nx.pagerank(graph, personalization={node: 1}).values()))\n",
    "        pr_vec = pr_vec / pagerank                                \n",
    "        vec_nce = np.where(pr_vec < 1, 0,                         \n",
    "                      np.log(pr_vec ))                            \n",
    "        similarity_matrix[:,repnodes.index(node)] = vec_nce\n",
    "        \n",
    "    # SVD decomposition\n",
    "    mat_u, vec_sigma, mat_v = ln.svd(similarity_matrix, full_matrices=False)\n",
    "    d = 50 if node_num >= 50 else node_num \n",
    "    diag = np.power(vec_sigma[0 : d], 0.5)\n",
    "    diag = diags(diag, 0)\n",
    "    embeddings = mat_u[:, 0 : d] @ diag\n",
    "    node_embeddings = torch.tensor(embeddings).to(torch.float32)\n",
    "#     print(similarity_matrix.shape)\n",
    "    \n",
    "    similarity_matrix = torch.tensor(similarity_matrix).float()   \n",
    "    labels_torch = torch.tensor(labels).long()\n",
    "    cls_num = len(np.unique(labels))\n",
    "    \n",
    "    run_num = 20\n",
    "    acc = 0\n",
    "    for _ in tqdm(range(run_num), desc='\\tTraining Iterations',ncols=50):\n",
    "        # Samples selection\n",
    "        node_random_sort = [i for i in range(n)]   \n",
    "        random.shuffle(node_random_sort)                                         \n",
    "        ## train and test samples\n",
    "        train_node = node_random_sort[0 : train_num]                             \n",
    "        test_node = node_random_sort[train_num :train_num + test_num]            \n",
    "\n",
    "        model = MultiLayerNet(input_size = d, num_classes = cls_num, dropout_rate = 0.3)\n",
    "        model.train()\n",
    "        model_train(model, node_embeddings[train_node], labels_torch[train_node])\n",
    "        model.eval()\n",
    "        acc += accuracy(model, node_embeddings[test_node], labels_torch[test_node])\n",
    "        \n",
    "    print('\\tNode classification accuracy: {:.2f}% with K = {}, threshold theta {}'.format((acc/run_num)*100, K, Theta))\n",
    "    \n",
    "    return round((acc/run_num)*100, 2)\n",
    "\n",
    "\n",
    "def anchor_nodes_marginal_test(graph_name):\n",
    "    \n",
    "     \n",
    "    graph, labels = graph_read(graph_name)\n",
    "    \n",
    "    anchors, anchor_mscore = anchor_nodes_marginal(graph, 500, 2)\n",
    "#     print(anchor_mscore)\n",
    "    \n",
    "    mscore_ave = [sum(anchor_mscore[i*20:i*20+20])/20 for i in range(25)]\n",
    "    print()\n",
    "    print(mscore_ave)\n",
    "    \n",
    "    print(first_less_theta_index(mscore_ave, 5)*20)\n",
    "    \n",
    "    return\n",
    "# anchor_nodes_marginal_test('cora')\n",
    "\n",
    "def marginal_effect_performance_eval(graph_name):\n",
    "    graph, lables = graph_read(graph_name)\n",
    "    Ks = [1, 2, 3, 4, 5]\n",
    "    Thetas = [16, 14, 12, 10, 8, 6, 4, 2]\n",
    "#     Ks = [1, 2]\n",
    "#     Thetas = [16, 14]\n",
    "    \n",
    "    result_mat = [[0] * len(Thetas) for _ in range(len(Ks))]\n",
    "    \n",
    "    for k in Ks:\n",
    "        print('\\n**** Damping factor K is {} **** '.format(k))\n",
    "        i = Ks.index(k)\n",
    "        for theta in Thetas:\n",
    "            j = Thetas.index(theta)\n",
    "            print('    ** Termination threshold theta is {} **'.format(theta))\n",
    "            result_mat[i][j] = marginal_effect_performance(graph, lables, K = k, Theta = theta, anchor_scale = 2000, Epsilon = 10**-3)\n",
    "        print() \n",
    "    \n",
    "    print(result_mat)\n",
    "    for i, row in enumerate(result_mat):\n",
    "        print(\"K is: {}, accuracy is: {} .\".format(i, row))\n",
    "\n",
    "    custom_x_ticks = [16, 14, 12, 10, 8, 6, 4, 2]\n",
    "#     custom_x_ticks = [16, 14]\n",
    "    \n",
    "    for i, row in enumerate(result_mat):\n",
    "        plt.plot(row, label=f'K =  {i+1}')\n",
    "\n",
    "#     plt.title('Five Lines Plot')\n",
    "    plt.xlabel(graph_name.capitalize()+' Threshold '+r'$\\theta$')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.xticks(range(len(custom_x_ticks)), custom_x_ticks)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# marginal_effect_performance_eval('cora')\n",
    "# marginal_effect_performance_eval('wiki')\n",
    "# marginal_effect_performance_eval('acm')\n",
    "# marginal_effect_performance_eval('citeseer')\n",
    "# marginal_effect_performance_eval('amap')    # epsilon = 10 **-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff4a612",
   "metadata": {},
   "source": [
    "## Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16e4b749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:45:10.013559Z",
     "start_time": "2024-07-24T07:45:09.995607Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import combinations\n",
    "\n",
    "def link_prediction(graph_name):\n",
    "    \n",
    "    \n",
    "    graph, _ = graph_read(graph_name)\n",
    "    \n",
    "    # edges selection for training and testing\n",
    "    nodes = list(graph.nodes)\n",
    "    edges = list(graph.edges) \n",
    "    num = int(len(edges)*0.2)\n",
    "    cls_num = 2\n",
    "    # node pairs selection\n",
    "    random.shuffle(edges)\n",
    "    pair_edges = edges[:num] # random selection\n",
    "    pair_non_edges = random.sample([p for p in itertools.combinations(nodes,2) if p not in graph.edges],num) \n",
    "    print('Pair selection finished!')\n",
    "    \n",
    "    # Samples selection\n",
    "    train_percent = 0.5\n",
    "    test_percent = 1 - train_percent\n",
    "    pair_num = num *2\n",
    "    train_num = int(pair_num * train_percent)\n",
    "    test_num  = int(pair_num * test_percent )\n",
    "    \n",
    "    pair_random_sort = [i for i in range(num*2)]   \n",
    "    random.shuffle(pair_random_sort)                                           \n",
    "    ## train sample\n",
    "    train_pair = pair_random_sort[0 : train_num]                                \n",
    "    test_pair = pair_random_sort[train_num : (train_num + test_num)]            \n",
    "    \n",
    "    \n",
    "    embeddings_matrix = rneet(graph, emb_dim = 50, K = 1, Theta = 14, Epsilon = 1e-2)\n",
    "    \n",
    "    \n",
    "    emb_dim = embeddings_matrix.shape[1]\n",
    "    \n",
    "    # Concatenate \n",
    "    pair_matrix = np.zeros((num*2, emb_dim*2))\n",
    "    is_edge = []\n",
    "    for i in range(num):\n",
    "        u_idx, v_idx = nodes.index(pair_edges[i][0]), nodes.index(pair_edges[i][1])\n",
    "        is_edge.append(1)\n",
    "        pair_matrix[2*i][0:emb_dim] = embeddings_matrix[u_idx]\n",
    "        pair_matrix[2*i][emb_dim:emb_dim*2] = embeddings_matrix[v_idx]\n",
    "\n",
    "        u_idx, v_idx = nodes.index(pair_non_edges[i][0]), nodes.index(pair_non_edges[i][1])\n",
    "        is_edge.append(0)\n",
    "        pair_matrix[2*i+1][0:emb_dim] = embeddings_matrix[u_idx]\n",
    "        pair_matrix[2*i+1][emb_dim:emb_dim*2] = embeddings_matrix[v_idx]\n",
    "    \n",
    "    pair_dim = pair_matrix.shape[1]\n",
    "    print('Link dimension is : {}'.format(pair_dim))\n",
    "    \n",
    "    pair_embeddings = torch.tensor(pair_matrix).to(torch.float32)\n",
    "    labels_torch = torch.tensor(is_edge).long()\n",
    "    run_num, acc = 10, 0.0\n",
    "    for _ in range(run_num):\n",
    "        model = MultiLayerNet(input_size = pair_dim, num_classes = cls_num, dropout_rate=0.3)\n",
    "        model.train()\n",
    "        model_train(model,pair_embeddings[train_pair], labels_torch[train_pair])\n",
    "        model.eval()\n",
    "        acc += accuracy(model, pair_embeddings[test_pair], labels_torch[test_pair])\n",
    "    \n",
    "    appro_acc = round(acc/run_num, 4)\n",
    "    print('Link prediction accuracy :{} with Concatenate'.format(appro_acc))\n",
    "            \n",
    "    # Hadmard Product                                              \n",
    "    pair_matrix = np.zeros((num*2, emb_dim))\n",
    "    is_edge = []\n",
    "    for i in range(num):\n",
    "        u_idx, v_idx = nodes.index(pair_edges[i][0]), nodes.index(pair_edges[i][1])\n",
    "        is_edge.append(1)\n",
    "        pair_matrix[2*i] = np.multiply(embeddings_matrix[u_idx], embeddings_matrix[v_idx])\n",
    "\n",
    "        u_idx, v_idx = nodes.index(pair_non_edges[i][0]), nodes.index(pair_non_edges[i][1])\n",
    "        is_edge.append(0)\n",
    "        pair_matrix[2*i+1] = np.multiply(embeddings_matrix[u_idx], embeddings_matrix[v_idx])\n",
    "            \n",
    "    pair_dim = pair_matrix.shape[1]\n",
    "    print('Link dimension is : {}'.format(pair_dim))\n",
    "    \n",
    "    pair_embeddings = torch.tensor(pair_matrix).to(torch.float32)\n",
    "    labels_torch = torch.tensor(is_edge).long()\n",
    "    run_num, acc = 10, 0.0\n",
    "    for _ in range(run_num):\n",
    "        model = MultiLayerNet(input_size = pair_dim, num_classes = cls_num, dropout_rate=0.3)\n",
    "        model.train()\n",
    "        model_train(model,pair_embeddings[train_pair], labels_torch[train_pair])\n",
    "        model.eval()\n",
    "        acc += accuracy(model, pair_embeddings[test_pair], labels_torch[test_pair])\n",
    "    \n",
    "    appro_acc = round(acc/run_num, 4)\n",
    "    print('Link prediction accuracy :{} with Hardamard Product'.format(appro_acc))\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def link_prediction_test():\n",
    "    \n",
    "    link_prediction('cora')\n",
    "    \n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fde29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a358bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "385.55px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
